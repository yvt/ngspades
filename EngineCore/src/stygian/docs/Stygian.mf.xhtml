<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><pre><![CDATA[

                        **Stygian Suppelemental Notes**

# Basics

## Coordinate space

:::
:::   o---> +X
:::   |
:::   v +Y
:::

## Quadrants

:::
:::    NW  |  NE        ',  Up  ,'
:::        |              ',  ,'
:::  ------o------     Left    Right
:::        |              ,'  ',
:::    SW  |  SE        ,' Down ',
:::

# Mipmapping

Stygian uses *mipmapping* for scalable implementation. From an input terrain, increasingly larger sets of the original data are generated. Each set is called a *mipmap level*. Usually, mipmap levels are structured so that the elements in each level are twice as large as the elements in the previous level ([!Figure mipmap]). For example, the second level (level 1) consists of 2×2-sized squares. The largest cell entirely covers the region.

[!Figure mipmap]: Mipmap levels.

    ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::
    :: +---+---+---+---+     +-------+-------+      +---------------+ ::
    :: |   |   |   |   |     |       |       |      |               | ::
    :: +---+---+---+---+     |       |       |      |               | ::
    :: |   |   |   |   |     |       |       |      |               | ::
    :: +---+---+---+---+     +-------+-------+      +---------------+ ::
    ::    Base level              Level 1                Level 2      ::
    ::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::::

This standard structure has a major issue with the opticast algorithm used by Stygian. Suppose we have a $256 \times 256 \times n$ terrain and we want to find a single downsampled cell covering the range $127 \leq x < 129, 127 \leq y < 129$. The size of the range is no larger than $2 \times 2$ but we have to use the last level (of the lowest resolution) because the range crosses cell boundaries in every level except the last one. For this reason, Stygian uses a specialized mipmapping scheme.

[!Figure mipmap2] shows the mipmapping scheme used by Stygian.

![!Figure mipmap2][](Images/Mipmap2.svg):
    The mipmap scheme used in Stygian.

The base level (level 0) is not used to simply the implementation and thus maximize the execution performance.

# Terrain rasterization

The terrain rasterizes creates a low-resolution depth image from terrain data and a scene setup. The generated image is guaranteed to have lower (farther from the camera) depth values compared to the ground truth image at every point, making itself useful for CPU-based occlusion culling.

The process goes as follows: (i) a panorama image around the camera is generated; (ii) the panorama image is projected to the final image using the camera's projection matrix.

## Sample allocation

Not every portion of the panorama image contributes to the final output as it may be outside the camera's viewport. Furthermore, the algorithm wouldn't be effective if the resolution of the panorama image is not well adjusted; for example, a too high resolution only harms the runtime performance without a much effect on the output quality. Naturally, the best result is achieved if the resolutions of the terrain data, the panorama image, and the final depth image are all consistent — one element (voxel/pixel) in one step roughly corresponds to another one in the subsequent step.

Each column in the panorama image is called *a beam* ([^Figure rast-beams]). Each beam corresponds to a particular range of azimuth angles, and serves as a basic unit on which a single run of the opticast algorithm is performed. A beam contains one or more *samples* for each of which the minimum (farthest from the camera) depth value is recorded. Each beam can have a varying number of samples.

![^Figure rast-beams][](Images/Opticast-Beams.svg):
    Beams and samples

The optimal sample allocation is determined using the two-level algorithm: (i) firstly, the distribution of beams is determined; (ii) secondly, samples are allocated to beams according to how much portion of beams is inside the viewport. The algorithm takes an image size $N$ as input, which applies scaling to sample counts in several portions of the algorithm. This value can be adjusted independently from the actual image size as needed.

The sample allocation algorithm requires the screen-space position of the zenith and nadir. In any case only one of the zenith and nadir is visible, and the visible one is called *a vanishing point*. A care must be taken when the optical axis of the camera is completely horizontal because in such cases a vanishing point is infinitely far away from the viewport ([!Figure rast-zenith]) and causes numerical problems if its screen-space position is stored explicitly. It's recommended that perspective division on its position is avoided entirely.

![!Figure rast-zenith][](Images/Opticast-Zenith.svg):
    The possible locations of a vanishing point

In screen-space, every longitudinal line is a straight line starting from the vanishing point. If one of such lines goes through the viewport, it must leave the viewport through a point on the viewport's border. Therefore, projecting all viewport edges oriented toward/against the vanishing point on the sphere and examining their spherical coordinates gives us an idea of which range of azimuth angles we must blanket with beams.

Alternatively, the direction of the change in the azimuth angle as you move on the edges could be examined to determine the orientations of the edges in regard to the vanishing point ([^Figure rast-azimuth-delta]). This method doesn't require the screen-space position of the vanishing point.

![^Figure rast-azimuth-delta][](Images/Opticast-AzimuthDelta.svg):
    An alternative way to obtain the range of azimuth angles

How wide each beam should be? One possible answer is to match the density of longitudinal lines to the output image resolution, i.e.:

$$
\left|
    \frac{\partial}{\partial\theta}Project(\vec{p})
\right|
\Delta\theta = \frac{2}{N}
$$

where $\Delta\theta$ is the width of a beam and $\vec{p}$ is a point with azimuth angle $\theta$.

The number of samples in a beam can be determined from the length of its portion inside the viewport.

## Opticast

The opticast is a procedure where beam-casting is performed on a quadrangle pyramid-shaped beam defined by two azimuth angles $\theta_1, \theta_2$ and two inclination angles $\varphi_1, \varphi_2$ against a terrain and a conservative depth value is obtained for each subdivided portion of a beam. Depth values are stored into a 1D array called *a beam depth buffer*.

Each beam is associated with a unique coordinate space called *a beam space*. A beam space is a local Cartesian space whose origin is located at the projection of the camera origin upon the plane $z = 0$, Z axis is parallel to the global Z axis, and X axis is oriented toward the azimuth angle $(\theta_1+\theta_2)/2$. The latitudinal line corresponding to this angle is called *a primary latitudinal line*.

*A beam projection matrix* describes the projective mapping between a beam space and a beam depth buffer. To project a beam-space point onto a beam depth buffer, the projective transformation described by a beam projection matrix is applied. After a perspective division, the Y component indicates the position (index) inside a beam depth buffer and is based on the distance from an endpoint of the visible portion of the primary latitudinal line in the viewport space. The Z component represents the depth value. The X component is not used. It's difficult to construct a beam projection matrix such that an unprojected pixel precisely covers the represented volumetric region as it's only accurate on $x_\text{beam}=0$. Use the mapping described in the next paragraph for precise results.

The precise mapping from the $i$-th pixel in a beam depth buffer of size $N_B$ to a volumetric region is defined as follows: (i) project two points whose spherical coordinates are $((\theta_1+\theta_2)/2, \varphi_1)$ and $((\theta_1+\theta_2)/2, \varphi_2)$ respectively to the viewport space; (ii) draw a line segment between the two projected points (iii) uniformly divide the line segment into $N_B$ segments; (iv) choose the $i$-th segment and let $p'_{i}, p'_{i+1}$ be its endpoints; (v) project them back to the world space and call them $p_i, p_{i+1}$; (vi) finally, define four planes ($\theta$ and $\varphi$ used here are azimuth and inclination angles, respectively): the first two are $\theta=\theta_1$, $\theta=\theta_2$ respectively and the others both include the line $\theta=(\theta_1+\theta_2)/2±\pi/2$, $\varphi=0$ and include $p_i$ and $p_{i+1}$, respectively. The said pixel is represented by the volumetric region enclosed by these four planes.

![!!Figure opticast-spans][](Images/Opticast-Span.svg):
    Rasterizing spans.

The 2D beam-casting produces a set of rows from an input terrain. Every produced row occpies entire the width of a beam as illustrated in [!!Figure opticast-spans]. The objective here is to project spans (sets of contiguous blocks aligned along the Z axis) onto a beam depth buffer. To do this, for each span, the largest rectangle is calculated ([!!Figure opticast-spans], dark shade). This is done by calculating the intersections of the span and the beam's left and right planes ([!!Figure opticast-spans], ●○) and choosing two of them ([!!Figure opticast-spans], ●) representing the smallest interval. The left and right edges of the rectangle may have different Z values. The difference is calculated using *a lateral projection matrix* and the smaller Z values are finally drawn to a beam depth buffer ([!!Figure opticast-spans], right).

## Rasterization

Finally, beam depth buffers are projected to the output depth image. There are several possible strategies based on the fundamental loop structure.

*Pixel*: The output pixels are iterated over and on each pixel one or more samples intersecting the pixel are sought. While this in principle provides a good writing memory access pattern, this is impractical for the current problem because the beam distribution is uneven and a binary search would be required to find a beam.

*Beam, Pixel*: Each beam is rasterized. Beams are *textured* with the depth values from the beam's samples. The requirement is a mapping from screen-space coordinates to beam depth buffer locations.

*Beam, Sample, Pixel*: Each sample is projected to the viewport and then rasterized. Since the overhead associated with rasterizing each polygon is somewhat high, it might be worth considering to fill an axis-aligned bounding box instead of filling the exact shape of a polygon.

The output is a downsampled depth image, i.e., each pixel's depth value must not be greater than the minimum value of actual depth values inside the pixel from an infinite-resolution render output. If a pixel is covered by multiple samples, they must be coalesced into one depth value by choosing the smallest depth value. Since it's guaranteed that samples covers the entirely of an output image, this can be implemented in rasterization schemes by the use of a *minimum value* blending mode.

# 2D beam-casting

The `mipbeamcast` function performs beam-casting with adaptive mipmapping on a given map.

The algorithm takes an *beam* as input. A beam is defined by one vertex `start` and direction vectors `dir1` and `dir2` of the two edges extending from the vertex ([^Figure beam]). The angle of the beam must be less than 45°. In reality, it's much smaller than that.

[^Figure beam]: A beam

    :::::::::::::::::::
    :: start    dir1 ::
    ::   o-------->  ::
    ::    \          ::
    ::     v         ::
    ::       dir2    ::
    :::::::::::::::::::

Another input is a *map* which is a 2D grid of size $N_1 \times N_2$ that spans across the region $\{(x, y) | 0 \leq x < N_1 \wedge 0 \leq y < N_2\}$. The map includes a set of *cells*, which are 1×1-sized squares that cover the region. In addition, the map also includes increasingly larger sets of cells. Each set is called a *mipmap level*. The exact structure of the mipmap levels is not fundamental to the algorithm and can be changed easily.

The goal of the algorithm is to find a set of cells such that every individual cell in the set entirely blocks the path of the beam. The secondary goal is to generate such a set without a gap between the cells.

## Axis normalization

To simplify the implementation, the algorithm transforms the input vectors so that `dir1` always falls into the SE-Right octant. The inverse transformation is applied to the algorithm's output to cancel the effect of axis normalization. After the transformation, `dir2` can be in one of the three octants ([^Figure dirrange]). Each case may require separate handling.

[^Figure dirrange]: The possible ranges of `dir1` and `dir2` after axis normalization

    :::::::::::::::::::::::::
    ::     ,'         ,'   ::
    ::   ,'         ,' ✓   ::
    ::  o-----     o-----  ::
    ::  |', ✓      |', ✓   ::
    ::  |  ',      |✓ ',   ::
    :::::::::::::::::::::::::

## Beam-casting

Each step of the main part of the algorithm proceeds as follows: (i) the portal through which the beam exits the current cell is determined; (ii) the smallest cell that intersects with or contains the portal, thus entirely occludes the path of the beam is found; (iii) move to the cell and repeat these steps until the beam exits the map. The loop invariant is that the entirety of the beam goes through the current cell.

Before entering the loop, the algorithm has to determine the first cell that occludes the beam. This is easy if `start` is inside the map — the base-level cell where `start` resides is what we are looking for. This is more complicated in the other cases. It's also possible that there is no such cell, e.g., when a part or the entirety of the beam misses the map. The output of the algorithm is empty in such cases.

|     |   $x_{start}$    |   $y_{start}$    |                                                            |
|-----|------------------|------------------|------------------------------------------------------------|
| (1) | $\geq N_1$       | any              | Empty output — never coincides with the map                |
| (2) | any              | $\geq N_2$       | Empty output — never or only partly coincides with the map |
| (3) | $0 \leq x < N_1$ | $< 0$            | Depends on `dir1` and `dir2`                               |
| (4) |                  | $0 \leq y < N_2$ | Starts inside the map                                      |
| (5) | $< 0$            | $< 0$            | Depends on `dir1` and `dir2`                               |
| (6) |                  | $0 \leq y < N_2$ | Depends on `dir1` and `dir2`                               |
# Terrain generation

Normally, terrain data is generated from polygonal geometry data.

The outline of the process is as following ([!!Figure voxel-overview]): (i) The input geometry is rasterized on the initial grid. (ii) Perform a flood fill starting from specified points, called *view points*. (iii) Invert the result of the previous step. (iv) Erode the result of the previous step by 1 block, cancelling the dilation effect of voxel rasterization done in the first step. (v) Finally, downsample the grid to the resolution suitable for run-time processing.

![!!Figure voxel-overview][](Images/Voxel-Overview.svg):
    The terrain generation process.

The process introduces two types of notable changes into the geometry: (a) *Hole filling*. Small gaps between objects are filled by the step (i) and they are not recovered by the erosion process of the step (iv). The maximum size of gaps affected by this effect is twice of the cell size. (b) *Occluder filtering*. The step (v) filters out small occluders. Furthermore, the mipmapping scheme used by Stygian affects the effective minimum occluder size. In the case of an axis-aligned occluder, the maximum size of the occluder size affected by this effect is $(3r + 2)s$ where `s` is the cell size and `r` is the reduction (downsampling) ratio.

The voxel grid used during the process (before downsampling) is called *an initial domain* ([!Figure voxel-inital-domain]). The size of an initial grid may be several times larger than the output size. The data density may be much higher as well because it may not be able to employ compression techniques like the final output does. For this reason, an initial domain is broken into regions called *tiles*. The memory usage of the process is kept in control by operating on a limited number of tiles at once.

![!Figure voxel-inital-domain][](Images/Voxel-InitialDomain.svg):
    An initial domain.

## Conservative triangle voxelization

The triangle voxelization algorithm finds the smallest set of columns whose union entirely contains a given triangle ([!Figure voxel-slice]). A column is defined as an axis-aligned box $x' \leq x < x'+1$, $y' \leq y < y'+1$, $z_1 \leq z < z_2$, $x', y' \in \mathbb{Z}$, $z_1, z_2 \in \mathbb{R}$.

![!Figure voxel-slice][](Images/Voxel-Slice.svg):
    Conservative triangle voxelization.

A naïve implementation would be based on a [linear programming][] algorithm. For each $x', y'$, the following linear program is solved:

$$\begin{aligned}
\text{Maximize or minimize} \ \ & z(\vec{x}) \\
\text{subject to} \ \  & x' \leq x < x'+1 \\
    & y' \leq y < y'+1 \\
    & \begin{bmatrix}p_1^T \\ p_2^T \\ p_3^T\end{bmatrix} \begin{bmatrix}x \\ y\end{bmatrix} \leq \begin{bmatrix}d_1 \\ d_2 \\ d_3\end{bmatrix}
\end{aligned}$$

If the minimum and maximum values exist, output them as $z_1$ and $z_2$ respectively. Otherwise (i.e., if some of the constraints are inconsistent), no column is generated.

[linear programming]: https://en.wikipedia.org/wiki/Linear_programming

Instead, Stygian implements a scanline-based algorithm. Firstly, the vertices of the input triangle is sorted by their Y coordinates. After that, the triangle is split into slices aligned with scanlines, each having a different shape depending on which of the vertices are in the scanline ([^Figure voxel-slice-types]). The coordinates of the vertices of the polygon forming each slice are evaluated. The maximum and minimum Z values of each cell can be found by clipping the slice's border edges to the cell and evaluating the Z coordinates of the vertices of the clipped edges. This gives a correct solution because the convex hull of the clipped edge is identical to the feasible region of the aforementioned linear program.

![^Figure voxel-slice-types][](Images/Voxel-SliceType.svg):
    The shapes of slices.

# Occlusion query

The clients query whether objects are occluded by occluders or not in order to skip the processing of objects which are invisible from a camera. A conservative depth buffer produced by terrain rasterization is used to quickly determine objects which are completely occluded by the terrain. This process is called *an occlusion query*.

The clients provide a convex four-dimensional polyhedron, which is specified using four or more vertices in clip-space coordinates. The query function returns `false` only if it's guaranteed that the polyhedron is completely occluded. Otherwise, it returns `true`. Because dealing with arbitrary polyhedra is complicated and computing-intensive, an input polyhedron is approximated using an AABB.

## Frustum clipping

Implemented by `clip_w_cs_aabb`.

This step removes some portion of the input AABB $Ω$ that does not intersect with the view volume. The view volume is defined by:

$$\begin{aligned}
-w_c \leq x_c \leq w_c \\
-w_c \leq y_c \leq w_c \\
0 \leq z_c \leq w_c
\end{aligned}$$

The subsequent step implicitly does frustum clipping after perspective division. The purpose of having a separate clipping step is to limit the range of the W values before perspective division with the goal of reducing the overestimation caused by taking an AABB of a post-division geometry. Consider the example where the input clip-space AABB stretches toward the negative W direction ([!Figure cull-w]).

![!Figure cull-w][](Images/Cull-W.svg):
    Clipping a clip-space AABB in the W direction.

The W values sent to the subsequent step must be restricted within the range $w \geq 0$ and should it touch $w = 0$, it must be positive zero.

]]></pre> <!-- Foremark footer -->
<script src="https://unpkg.com/foremark/browser/foremark.js" async="async" /></html>